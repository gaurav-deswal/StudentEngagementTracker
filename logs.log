2024-05-12 14:30:20,085:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-12 14:30:20,085:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-12 14:30:20,085:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-12 14:30:20,085:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-12 14:30:33,047:INFO:PyCaret ClassificationExperiment
2024-05-12 14:30:33,048:INFO:Logging name: clf-default-name
2024-05-12 14:30:33,048:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-05-12 14:30:33,049:INFO:version 3.3.2
2024-05-12 14:30:33,049:INFO:Initializing setup()
2024-05-12 14:30:33,049:INFO:self.USI: 77b9
2024-05-12 14:30:33,049:INFO:self._variable_keys: {'fold_shuffle_param', '_ml_usecase', 'n_jobs_param', 'data', 'X_test', 'gpu_param', 'y_test', 'is_multiclass', 'gpu_n_jobs_param', 'logging_param', 'pipeline', 'log_plots_param', 'target_param', 'X_train', 'USI', 'seed', 'X', 'y', 'exp_id', 'html_param', 'y_train', 'fold_groups_param', 'idx', '_available_plots', 'memory', 'exp_name_log', 'fix_imbalance', 'fold_generator'}
2024-05-12 14:30:33,049:INFO:Checking environment
2024-05-12 14:30:33,049:INFO:python_version: 3.11.8
2024-05-12 14:30:33,049:INFO:python_build: ('main', 'Feb 26 2024 21:34:05')
2024-05-12 14:30:33,049:INFO:machine: AMD64
2024-05-12 14:30:33,049:INFO:platform: Windows-10-10.0.22631-SP0
2024-05-12 14:30:33,050:INFO:Memory: svmem(total=16371511296, available=1342615552, percent=91.8, used=15028895744, free=1342615552)
2024-05-12 14:30:33,050:INFO:Physical Core: 8
2024-05-12 14:30:33,050:INFO:Logical Core: 16
2024-05-12 14:30:33,050:INFO:Checking libraries
2024-05-12 14:30:33,050:INFO:System:
2024-05-12 14:30:33,050:INFO:    python: 3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]
2024-05-12 14:30:33,050:INFO:executable: c:\Users\Manvendra Nema\anaconda3\envs\vercil\python.exe
2024-05-12 14:30:33,050:INFO:   machine: Windows-10-10.0.22631-SP0
2024-05-12 14:30:33,050:INFO:PyCaret required dependencies:
2024-05-12 14:30:33,442:INFO:                 pip: 23.3.1
2024-05-12 14:30:33,442:INFO:          setuptools: 68.2.2
2024-05-12 14:30:33,442:INFO:             pycaret: 3.3.2
2024-05-12 14:30:33,442:INFO:             IPython: 8.20.0
2024-05-12 14:30:33,442:INFO:          ipywidgets: 8.1.2
2024-05-12 14:30:33,442:INFO:                tqdm: 4.64.1
2024-05-12 14:30:33,442:INFO:               numpy: 1.24.3
2024-05-12 14:30:33,442:INFO:              pandas: 2.2.1
2024-05-12 14:30:33,442:INFO:              jinja2: 3.1.3
2024-05-12 14:30:33,442:INFO:               scipy: 1.11.4
2024-05-12 14:30:33,442:INFO:              joblib: 1.2.0
2024-05-12 14:30:33,442:INFO:             sklearn: 1.4.2
2024-05-12 14:30:33,442:INFO:                pyod: 1.1.3
2024-05-12 14:30:33,442:INFO:            imblearn: 0.12.2
2024-05-12 14:30:33,442:INFO:   category_encoders: 2.6.3
2024-05-12 14:30:33,442:INFO:            lightgbm: 4.3.0
2024-05-12 14:30:33,442:INFO:               numba: 0.59.1
2024-05-12 14:30:33,442:INFO:            requests: 2.31.0
2024-05-12 14:30:33,442:INFO:          matplotlib: 3.7.5
2024-05-12 14:30:33,442:INFO:          scikitplot: 0.3.7
2024-05-12 14:30:33,442:INFO:         yellowbrick: 1.5
2024-05-12 14:30:33,442:INFO:              plotly: 5.22.0
2024-05-12 14:30:33,442:INFO:    plotly-resampler: Not installed
2024-05-12 14:30:33,442:INFO:             kaleido: 0.2.1
2024-05-12 14:30:33,442:INFO:           schemdraw: 0.15
2024-05-12 14:30:33,442:INFO:         statsmodels: 0.14.2
2024-05-12 14:30:33,442:INFO:              sktime: 0.26.0
2024-05-12 14:30:33,442:INFO:               tbats: 1.1.3
2024-05-12 14:30:33,442:INFO:            pmdarima: 2.0.4
2024-05-12 14:30:33,442:INFO:              psutil: 5.9.0
2024-05-12 14:30:33,442:INFO:          markupsafe: 2.1.3
2024-05-12 14:30:33,442:INFO:             pickle5: Not installed
2024-05-12 14:30:33,442:INFO:         cloudpickle: 2.2.1
2024-05-12 14:30:33,442:INFO:         deprecation: 2.1.0
2024-05-12 14:30:33,442:INFO:              xxhash: 2.0.2
2024-05-12 14:30:33,442:INFO:           wurlitzer: Not installed
2024-05-12 14:30:33,442:INFO:PyCaret optional dependencies:
2024-05-12 14:30:33,463:INFO:                shap: Not installed
2024-05-12 14:30:33,463:INFO:           interpret: Not installed
2024-05-12 14:30:33,463:INFO:                umap: Not installed
2024-05-12 14:30:33,463:INFO:     ydata_profiling: Not installed
2024-05-12 14:30:33,463:INFO:  explainerdashboard: Not installed
2024-05-12 14:30:33,463:INFO:             autoviz: Not installed
2024-05-12 14:30:33,463:INFO:           fairlearn: Not installed
2024-05-12 14:30:33,463:INFO:          deepchecks: Not installed
2024-05-12 14:30:33,463:INFO:             xgboost: Not installed
2024-05-12 14:30:33,463:INFO:            catboost: Not installed
2024-05-12 14:30:33,463:INFO:              kmodes: Not installed
2024-05-12 14:30:33,463:INFO:             mlxtend: Not installed
2024-05-12 14:30:33,463:INFO:       statsforecast: Not installed
2024-05-12 14:30:33,463:INFO:        tune_sklearn: Not installed
2024-05-12 14:30:33,463:INFO:                 ray: Not installed
2024-05-12 14:30:33,463:INFO:            hyperopt: Not installed
2024-05-12 14:30:33,463:INFO:              optuna: Not installed
2024-05-12 14:30:33,463:INFO:               skopt: Not installed
2024-05-12 14:30:33,463:INFO:              mlflow: Not installed
2024-05-12 14:30:33,463:INFO:              gradio: Not installed
2024-05-12 14:30:33,463:INFO:             fastapi: Not installed
2024-05-12 14:30:33,463:INFO:             uvicorn: Not installed
2024-05-12 14:30:33,463:INFO:              m2cgen: Not installed
2024-05-12 14:30:33,463:INFO:           evidently: Not installed
2024-05-12 14:30:33,463:INFO:               fugue: Not installed
2024-05-12 14:30:33,463:INFO:           streamlit: Not installed
2024-05-12 14:30:33,463:INFO:             prophet: Not installed
2024-05-12 14:30:33,463:INFO:None
2024-05-12 14:30:33,463:INFO:Set up data.
2024-05-12 14:30:36,245:INFO:Set up folding strategy.
2024-05-12 14:30:36,245:INFO:Set up train/test split.
2024-05-12 14:30:49,207:INFO:Set up index.
2024-05-12 14:30:49,254:INFO:Assigning column types.
2024-05-12 14:30:51,115:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-05-12 14:30:51,177:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-12 14:30:51,200:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-12 14:30:51,261:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,262:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,312:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-12 14:30:51,312:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-12 14:30:51,345:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,346:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,346:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-05-12 14:30:51,394:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-12 14:30:51,427:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,428:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,480:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-12 14:30:51,518:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,518:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,518:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-05-12 14:30:51,595:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,595:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,683:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,683:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:30:51,693:INFO:Preparing preprocessing pipeline...
2024-05-12 14:30:51,927:INFO:Set up simple imputation.
2024-05-12 14:30:51,928:INFO:Set up imbalanced handling.
2024-05-12 14:30:51,928:INFO:Set up feature normalization.
2024-05-12 14:31:04,524:INFO:Finished creating preprocessing pipeline.
2024-05-12 14:31:04,532:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\MANVEN~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2024-05-12 14:31:04,532:INFO:Creating final display dataframe.
2024-05-12 14:31:55,226:INFO:Setup _display_container:                     Description             Value
0                    Session id              2681
1                        Target            target
2                   Target type        Multiclass
3           Original data shape      (79066, 710)
4        Transformed data shape     (135564, 710)
5   Transformed train set shape     (111844, 710)
6    Transformed test set shape      (23720, 710)
7              Numeric features               709
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                Fix imbalance              True
13         Fix imbalance method             SMOTE
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              77b9
2024-05-12 14:31:55,232:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\IPython\core\formatters.py:344: FutureWarning: Index.format is deprecated and will be removed in a future version. Convert using index.astype(str) or index.map(formatter) instead.
  return method()

2024-05-12 14:31:55,232:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\IPython\core\formatters.py:344: FutureWarning: RangeIndex.format is deprecated and will be removed in a future version. Convert using index.astype(str) or index.map(formatter) instead.
  return method()

2024-05-12 14:31:55,315:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:31:55,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:31:55,406:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:31:55,406:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 14:31:55,406:INFO:setup() successfully completed in 82.36s...............
2024-05-12 14:32:32,547:INFO:Initializing compare_models()
2024-05-12 14:32:32,547:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C41A1DDF90>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001C41A1DDF90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-05-12 14:32:32,547:INFO:Checking exceptions
2024-05-12 14:32:33,584:INFO:Preparing display monitor
2024-05-12 14:32:33,620:INFO:Initializing Logistic Regression
2024-05-12 14:32:33,621:INFO:Total runtime is 1.7555554707845054e-05 minutes
2024-05-12 14:32:33,622:INFO:SubProcess create_model() called ==================================
2024-05-12 14:32:33,622:INFO:Initializing create_model()
2024-05-12 14:32:33,626:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C41A1DDF90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C4462373D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 14:32:33,626:INFO:Checking exceptions
2024-05-12 14:32:33,626:INFO:Importing libraries
2024-05-12 14:32:33,626:INFO:Copying training dataset
2024-05-12 14:32:35,498:INFO:Defining folds
2024-05-12 14:32:35,498:INFO:Declaring metric variables
2024-05-12 14:32:35,500:INFO:Importing untrained model
2024-05-12 14:32:35,500:INFO:Logistic Regression Imported successfully
2024-05-12 14:32:35,511:INFO:Starting cross validation
2024-05-12 14:32:35,537:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-05-12 14:32:47,194:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.1.4', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.2.1', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
  warnings.warn(

2024-05-12 14:32:47,194:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.1.4', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.2.1', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
  warnings.warn(

2024-05-12 14:32:47,195:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.1.4', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.2.1', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
  warnings.warn(

2024-05-12 14:32:47,195:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.1.4', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.2.1', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
  warnings.warn(

2024-05-12 14:32:47,195:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.1.4', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.2.1', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
  warnings.warn(

2024-05-12 14:32:47,195:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.1.4', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.2.1', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
  warnings.warn(

2024-05-12 14:32:47,195:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.1.4', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.2.1', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
  warnings.warn(

2024-05-12 14:32:47,195:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.1.4', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.2.1', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
  warnings.warn(

2024-05-12 14:32:47,208:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.1.4', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.3.1', 'setuptools': '68.2.2', 'pycaret': '3.3.2', 'IPython': '8.20.0', 'ipywidgets': '8.1.2', 'tqdm': '4.64.1', 'numpy': '1.24.3', 'pandas': '2.2.1', 'jinja2': '3.1.3', 'scipy': '1.11.4', 'joblib': '1.2.0', 'sklearn': '1.4.2', 'pyod': '1.1.3', 'imblearn': '0.12.2', 'category_encoders': '2.6.3', 'lightgbm': '4.3.0', 'numba': '0.59.1', 'requests': '2.31.0', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.22.0', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.2', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '5.9.0', 'markupsafe': '2.1.3', 'pickle5': 'Not installed', 'cloudpickle': '2.2.1', 'deprecation': '2.1.0', 'xxhash': '2.0.2', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.8', 'machine': 'AMD64'}}
  warnings.warn(

2024-05-12 15:19:11,973:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-12 15:19:11,973:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-12 15:19:11,973:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-12 15:19:11,973:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-12 15:19:12,583:INFO:PyCaret ClassificationExperiment
2024-05-12 15:19:12,583:INFO:Logging name: clf-default-name
2024-05-12 15:19:12,583:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-05-12 15:19:12,583:INFO:version 3.3.2
2024-05-12 15:19:12,583:INFO:Initializing setup()
2024-05-12 15:19:12,583:INFO:self.USI: da87
2024-05-12 15:19:12,583:INFO:self._variable_keys: {'seed', 'y_train', 'y', 'gpu_n_jobs_param', 'fold_shuffle_param', 'log_plots_param', 'fix_imbalance', 'USI', 'y_test', 'idx', 'n_jobs_param', '_ml_usecase', '_available_plots', 'exp_name_log', 'exp_id', 'X_train', 'X_test', 'html_param', 'is_multiclass', 'target_param', 'pipeline', 'gpu_param', 'logging_param', 'fold_generator', 'memory', 'X', 'data', 'fold_groups_param'}
2024-05-12 15:19:12,583:INFO:Checking environment
2024-05-12 15:19:12,583:INFO:python_version: 3.11.8
2024-05-12 15:19:12,583:INFO:python_build: ('main', 'Feb 26 2024 21:34:05')
2024-05-12 15:19:12,583:INFO:machine: AMD64
2024-05-12 15:19:12,583:INFO:platform: Windows-10-10.0.22631-SP0
2024-05-12 15:19:12,583:INFO:Memory: svmem(total=16371511296, available=4311904256, percent=73.7, used=12059607040, free=4311904256)
2024-05-12 15:19:12,583:INFO:Physical Core: 8
2024-05-12 15:19:12,583:INFO:Logical Core: 16
2024-05-12 15:19:12,583:INFO:Checking libraries
2024-05-12 15:19:12,583:INFO:System:
2024-05-12 15:19:12,583:INFO:    python: 3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]
2024-05-12 15:19:12,583:INFO:executable: c:\Users\Manvendra Nema\anaconda3\envs\vercil\python.exe
2024-05-12 15:19:12,583:INFO:   machine: Windows-10-10.0.22631-SP0
2024-05-12 15:19:12,583:INFO:PyCaret required dependencies:
2024-05-12 15:19:12,673:INFO:                 pip: 23.3.1
2024-05-12 15:19:12,673:INFO:          setuptools: 68.2.2
2024-05-12 15:19:12,673:INFO:             pycaret: 3.3.2
2024-05-12 15:19:12,673:INFO:             IPython: 8.20.0
2024-05-12 15:19:12,673:INFO:          ipywidgets: 8.1.2
2024-05-12 15:19:12,673:INFO:                tqdm: 4.64.1
2024-05-12 15:19:12,673:INFO:               numpy: 1.24.3
2024-05-12 15:19:12,673:INFO:              pandas: 2.1.4
2024-05-12 15:19:12,673:INFO:              jinja2: 3.1.3
2024-05-12 15:19:12,673:INFO:               scipy: 1.11.4
2024-05-12 15:19:12,673:INFO:              joblib: 1.2.0
2024-05-12 15:19:12,673:INFO:             sklearn: 1.4.2
2024-05-12 15:19:12,673:INFO:                pyod: 1.1.3
2024-05-12 15:19:12,673:INFO:            imblearn: 0.12.2
2024-05-12 15:19:12,673:INFO:   category_encoders: 2.6.3
2024-05-12 15:19:12,673:INFO:            lightgbm: 4.3.0
2024-05-12 15:19:12,673:INFO:               numba: 0.59.1
2024-05-12 15:19:12,673:INFO:            requests: 2.31.0
2024-05-12 15:19:12,673:INFO:          matplotlib: 3.7.5
2024-05-12 15:19:12,673:INFO:          scikitplot: 0.3.7
2024-05-12 15:19:12,673:INFO:         yellowbrick: 1.5
2024-05-12 15:19:12,673:INFO:              plotly: 5.22.0
2024-05-12 15:19:12,673:INFO:    plotly-resampler: Not installed
2024-05-12 15:19:12,673:INFO:             kaleido: 0.2.1
2024-05-12 15:19:12,673:INFO:           schemdraw: 0.15
2024-05-12 15:19:12,673:INFO:         statsmodels: 0.14.2
2024-05-12 15:19:12,673:INFO:              sktime: 0.26.0
2024-05-12 15:19:12,673:INFO:               tbats: 1.1.3
2024-05-12 15:19:12,673:INFO:            pmdarima: 2.0.4
2024-05-12 15:19:12,673:INFO:              psutil: 5.9.0
2024-05-12 15:19:12,673:INFO:          markupsafe: 2.1.3
2024-05-12 15:19:12,673:INFO:             pickle5: Not installed
2024-05-12 15:19:12,673:INFO:         cloudpickle: 2.2.1
2024-05-12 15:19:12,673:INFO:         deprecation: 2.1.0
2024-05-12 15:19:12,673:INFO:              xxhash: 2.0.2
2024-05-12 15:19:12,673:INFO:           wurlitzer: Not installed
2024-05-12 15:19:12,673:INFO:PyCaret optional dependencies:
2024-05-12 15:19:12,683:INFO:                shap: Not installed
2024-05-12 15:19:12,683:INFO:           interpret: Not installed
2024-05-12 15:19:12,683:INFO:                umap: Not installed
2024-05-12 15:19:12,683:INFO:     ydata_profiling: Not installed
2024-05-12 15:19:12,683:INFO:  explainerdashboard: Not installed
2024-05-12 15:19:12,683:INFO:             autoviz: Not installed
2024-05-12 15:19:12,683:INFO:           fairlearn: Not installed
2024-05-12 15:19:12,683:INFO:          deepchecks: Not installed
2024-05-12 15:19:12,683:INFO:             xgboost: Not installed
2024-05-12 15:19:12,683:INFO:            catboost: Not installed
2024-05-12 15:19:12,683:INFO:              kmodes: Not installed
2024-05-12 15:19:12,683:INFO:             mlxtend: Not installed
2024-05-12 15:19:12,683:INFO:       statsforecast: Not installed
2024-05-12 15:19:12,683:INFO:        tune_sklearn: Not installed
2024-05-12 15:19:12,683:INFO:                 ray: Not installed
2024-05-12 15:19:12,683:INFO:            hyperopt: Not installed
2024-05-12 15:19:12,683:INFO:              optuna: Not installed
2024-05-12 15:19:12,683:INFO:               skopt: Not installed
2024-05-12 15:19:12,683:INFO:              mlflow: Not installed
2024-05-12 15:19:12,683:INFO:              gradio: Not installed
2024-05-12 15:19:12,683:INFO:             fastapi: Not installed
2024-05-12 15:19:12,683:INFO:             uvicorn: Not installed
2024-05-12 15:19:12,683:INFO:              m2cgen: Not installed
2024-05-12 15:19:12,683:INFO:           evidently: Not installed
2024-05-12 15:19:12,683:INFO:               fugue: Not installed
2024-05-12 15:19:12,683:INFO:           streamlit: Not installed
2024-05-12 15:19:12,683:INFO:             prophet: Not installed
2024-05-12 15:19:12,683:INFO:None
2024-05-12 15:19:12,683:INFO:Set up data.
2024-05-12 15:19:13,344:INFO:Set up folding strategy.
2024-05-12 15:19:13,344:INFO:Set up train/test split.
2024-05-12 15:19:14,704:INFO:Set up index.
2024-05-12 15:19:14,744:INFO:Assigning column types.
2024-05-12 15:19:16,504:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-05-12 15:19:16,554:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-12 15:19:16,564:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-12 15:19:16,604:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,604:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,634:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-05-12 15:19:16,634:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-12 15:19:16,664:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,664:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,664:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-05-12 15:19:16,704:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-12 15:19:16,734:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,734:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,774:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-05-12 15:19:16,804:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,804:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-05-12 15:19:16,894:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,894:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,974:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,974:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:19:16,974:INFO:Preparing preprocessing pipeline...
2024-05-12 15:19:17,204:INFO:Set up simple imputation.
2024-05-12 15:19:17,204:INFO:Set up imbalanced handling.
2024-05-12 15:19:17,204:INFO:Set up feature normalization.
2024-05-12 15:19:29,973:INFO:Finished creating preprocessing pipeline.
2024-05-12 15:19:29,973:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\MANVEN~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2024-05-12 15:19:29,983:INFO:Creating final display dataframe.
2024-05-12 15:20:11,390:INFO:Setup _display_container:                     Description             Value
0                    Session id              3090
1                        Target            target
2                   Target type        Multiclass
3           Original data shape      (79066, 710)
4        Transformed data shape     (135564, 710)
5   Transformed train set shape     (111844, 710)
6    Transformed test set shape      (23720, 710)
7              Numeric features               709
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                Fix imbalance              True
13         Fix imbalance method             SMOTE
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                 2
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              da87
2024-05-12 15:20:11,470:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:20:11,470:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:20:11,550:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:20:11,550:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-05-12 15:20:11,550:INFO:setup() successfully completed in 58.98s...............
2024-05-12 15:20:11,554:INFO:Initializing compare_models()
2024-05-12 15:20:11,554:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-05-12 15:20:11,554:INFO:Checking exceptions
2024-05-12 15:20:12,441:INFO:Preparing display monitor
2024-05-12 15:20:12,471:INFO:Initializing Logistic Regression
2024-05-12 15:20:12,471:INFO:Total runtime is 0.0 minutes
2024-05-12 15:20:12,471:INFO:SubProcess create_model() called ==================================
2024-05-12 15:20:12,471:INFO:Initializing create_model()
2024-05-12 15:20:12,471:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002879ABB57D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 15:20:12,471:INFO:Checking exceptions
2024-05-12 15:20:12,471:INFO:Importing libraries
2024-05-12 15:20:12,471:INFO:Copying training dataset
2024-05-12 15:20:13,846:INFO:Defining folds
2024-05-12 15:20:13,846:INFO:Declaring metric variables
2024-05-12 15:20:13,846:INFO:Importing untrained model
2024-05-12 15:20:13,856:INFO:Logistic Regression Imported successfully
2024-05-12 15:20:13,863:INFO:Starting cross validation
2024-05-12 15:20:13,876:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 15:22:10,497:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-05-12 15:22:10,873:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:22:12,155:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-05-12 15:22:12,504:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:23:58,793:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-05-12 15:23:59,034:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:24:02,723:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-05-12 15:24:03,128:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:25:48,819:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-05-12 15:25:49,094:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:25:50,627:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-05-12 15:25:50,853:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:27:35,712:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-05-12 15:27:35,978:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:27:40,833:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-05-12 15:27:41,186:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:29:23,412:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-05-12 15:29:23,649:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:29:27,653:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-05-12 15:29:27,850:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:29:27,891:INFO:Calculating mean and std
2024-05-12 15:29:27,892:INFO:Creating metrics dataframe
2024-05-12 15:29:27,896:INFO:Uploading results into container
2024-05-12 15:29:27,896:INFO:Uploading model into container now
2024-05-12 15:29:27,896:INFO:_master_model_container: 1
2024-05-12 15:29:27,896:INFO:_display_container: 2
2024-05-12 15:29:27,896:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=3090, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-05-12 15:29:27,896:INFO:create_model() successfully completed......................................
2024-05-12 15:29:28,104:INFO:SubProcess create_model() end ==================================
2024-05-12 15:29:28,104:INFO:Creating metrics dataframe
2024-05-12 15:29:28,113:INFO:Initializing K Neighbors Classifier
2024-05-12 15:29:28,113:INFO:Total runtime is 9.260694559415182 minutes
2024-05-12 15:29:28,115:INFO:SubProcess create_model() called ==================================
2024-05-12 15:29:28,115:INFO:Initializing create_model()
2024-05-12 15:29:28,116:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002879ABB57D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 15:29:28,116:INFO:Checking exceptions
2024-05-12 15:29:28,116:INFO:Importing libraries
2024-05-12 15:29:28,116:INFO:Copying training dataset
2024-05-12 15:29:29,521:INFO:Defining folds
2024-05-12 15:29:29,521:INFO:Declaring metric variables
2024-05-12 15:29:29,531:INFO:Importing untrained model
2024-05-12 15:29:29,536:INFO:K Neighbors Classifier Imported successfully
2024-05-12 15:29:29,544:INFO:Starting cross validation
2024-05-12 15:29:29,566:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 15:32:22,772:INFO:Calculating mean and std
2024-05-12 15:32:22,787:INFO:Creating metrics dataframe
2024-05-12 15:32:22,792:INFO:Uploading results into container
2024-05-12 15:32:22,793:INFO:Uploading model into container now
2024-05-12 15:32:22,793:INFO:_master_model_container: 2
2024-05-12 15:32:22,793:INFO:_display_container: 2
2024-05-12 15:32:22,794:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=2, n_neighbors=5, p=2,
                     weights='uniform')
2024-05-12 15:32:22,794:INFO:create_model() successfully completed......................................
2024-05-12 15:32:23,026:INFO:SubProcess create_model() end ==================================
2024-05-12 15:32:23,026:INFO:Creating metrics dataframe
2024-05-12 15:32:23,037:INFO:Initializing Naive Bayes
2024-05-12 15:32:23,037:INFO:Total runtime is 12.17609867254893 minutes
2024-05-12 15:32:23,037:INFO:SubProcess create_model() called ==================================
2024-05-12 15:32:23,037:INFO:Initializing create_model()
2024-05-12 15:32:23,037:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002879ABB57D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 15:32:23,037:INFO:Checking exceptions
2024-05-12 15:32:23,037:INFO:Importing libraries
2024-05-12 15:32:23,037:INFO:Copying training dataset
2024-05-12 15:32:24,454:INFO:Defining folds
2024-05-12 15:32:24,454:INFO:Declaring metric variables
2024-05-12 15:32:24,454:INFO:Importing untrained model
2024-05-12 15:32:24,461:INFO:Naive Bayes Imported successfully
2024-05-12 15:32:24,468:INFO:Starting cross validation
2024-05-12 15:32:24,486:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 15:33:40,529:INFO:Calculating mean and std
2024-05-12 15:33:40,530:INFO:Creating metrics dataframe
2024-05-12 15:33:40,534:INFO:Uploading results into container
2024-05-12 15:33:40,534:INFO:Uploading model into container now
2024-05-12 15:33:40,535:INFO:_master_model_container: 3
2024-05-12 15:33:40,536:INFO:_display_container: 2
2024-05-12 15:33:40,536:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-05-12 15:33:40,536:INFO:create_model() successfully completed......................................
2024-05-12 15:33:40,725:INFO:SubProcess create_model() end ==================================
2024-05-12 15:33:40,725:INFO:Creating metrics dataframe
2024-05-12 15:33:40,732:INFO:Initializing Decision Tree Classifier
2024-05-12 15:33:40,733:INFO:Total runtime is 13.471032122770945 minutes
2024-05-12 15:33:40,736:INFO:SubProcess create_model() called ==================================
2024-05-12 15:33:40,737:INFO:Initializing create_model()
2024-05-12 15:33:40,737:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002879ABB57D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 15:33:40,737:INFO:Checking exceptions
2024-05-12 15:33:40,737:INFO:Importing libraries
2024-05-12 15:33:40,737:INFO:Copying training dataset
2024-05-12 15:33:42,136:INFO:Defining folds
2024-05-12 15:33:42,136:INFO:Declaring metric variables
2024-05-12 15:33:42,140:INFO:Importing untrained model
2024-05-12 15:33:42,144:INFO:Decision Tree Classifier Imported successfully
2024-05-12 15:33:42,149:INFO:Starting cross validation
2024-05-12 15:33:42,167:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 15:46:29,016:INFO:Calculating mean and std
2024-05-12 15:46:29,016:INFO:Creating metrics dataframe
2024-05-12 15:46:29,024:INFO:Uploading results into container
2024-05-12 15:46:29,024:INFO:Uploading model into container now
2024-05-12 15:46:29,024:INFO:_master_model_container: 4
2024-05-12 15:46:29,024:INFO:_display_container: 2
2024-05-12 15:46:29,024:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=3090, splitter='best')
2024-05-12 15:46:29,024:INFO:create_model() successfully completed......................................
2024-05-12 15:46:29,212:INFO:SubProcess create_model() end ==================================
2024-05-12 15:46:29,212:INFO:Creating metrics dataframe
2024-05-12 15:46:29,221:INFO:Initializing SVM - Linear Kernel
2024-05-12 15:46:29,221:INFO:Total runtime is 26.279155166943866 minutes
2024-05-12 15:46:29,221:INFO:SubProcess create_model() called ==================================
2024-05-12 15:46:29,221:INFO:Initializing create_model()
2024-05-12 15:46:29,221:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002879ABB57D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 15:46:29,221:INFO:Checking exceptions
2024-05-12 15:46:29,221:INFO:Importing libraries
2024-05-12 15:46:29,221:INFO:Copying training dataset
2024-05-12 15:46:30,596:INFO:Defining folds
2024-05-12 15:46:30,596:INFO:Declaring metric variables
2024-05-12 15:46:30,596:INFO:Importing untrained model
2024-05-12 15:46:30,604:INFO:SVM - Linear Kernel Imported successfully
2024-05-12 15:46:30,612:INFO:Starting cross validation
2024-05-12 15:46:30,629:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 15:47:21,472:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:47:23,258:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:48:10,907:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:48:12,358:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:49:00,235:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:49:01,338:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:49:49,790:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:49:50,474:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:50:39,337:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:50:41,130:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:50:41,150:INFO:Calculating mean and std
2024-05-12 15:50:41,153:INFO:Creating metrics dataframe
2024-05-12 15:50:41,157:INFO:Uploading results into container
2024-05-12 15:50:41,157:INFO:Uploading model into container now
2024-05-12 15:50:41,157:INFO:_master_model_container: 5
2024-05-12 15:50:41,157:INFO:_display_container: 2
2024-05-12 15:50:41,157:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=2, penalty='l2',
              power_t=0.5, random_state=3090, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-05-12 15:50:41,157:INFO:create_model() successfully completed......................................
2024-05-12 15:50:41,349:INFO:SubProcess create_model() end ==================================
2024-05-12 15:50:41,349:INFO:Creating metrics dataframe
2024-05-12 15:50:41,354:INFO:Initializing Ridge Classifier
2024-05-12 15:50:41,354:INFO:Total runtime is 30.481378400325774 minutes
2024-05-12 15:50:41,358:INFO:SubProcess create_model() called ==================================
2024-05-12 15:50:41,358:INFO:Initializing create_model()
2024-05-12 15:50:41,358:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002879ABB57D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 15:50:41,358:INFO:Checking exceptions
2024-05-12 15:50:41,358:INFO:Importing libraries
2024-05-12 15:50:41,358:INFO:Copying training dataset
2024-05-12 15:50:42,655:INFO:Defining folds
2024-05-12 15:50:42,655:INFO:Declaring metric variables
2024-05-12 15:50:42,657:INFO:Importing untrained model
2024-05-12 15:50:42,663:INFO:Ridge Classifier Imported successfully
2024-05-12 15:50:42,668:INFO:Starting cross validation
2024-05-12 15:50:42,686:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 15:51:00,465:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:51:01,504:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:51:15,101:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=7.21478e-11): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-05-12 15:51:15,434:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:51:15,567:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=1.78734e-11): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-05-12 15:51:15,899:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:51:34,017:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:51:34,121:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:51:49,110:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.08826e-10): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-05-12 15:51:49,370:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:51:51,440:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:52:08,693:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:52:09,323:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 15:52:09,349:INFO:Calculating mean and std
2024-05-12 15:52:09,349:INFO:Creating metrics dataframe
2024-05-12 15:52:09,349:INFO:Uploading results into container
2024-05-12 15:52:09,349:INFO:Uploading model into container now
2024-05-12 15:52:09,356:INFO:_master_model_container: 6
2024-05-12 15:52:09,356:INFO:_display_container: 2
2024-05-12 15:52:09,356:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=3090, solver='auto',
                tol=0.0001)
2024-05-12 15:52:09,356:INFO:create_model() successfully completed......................................
2024-05-12 15:52:09,573:INFO:SubProcess create_model() end ==================================
2024-05-12 15:52:09,573:INFO:Creating metrics dataframe
2024-05-12 15:52:09,578:INFO:Initializing Random Forest Classifier
2024-05-12 15:52:09,578:INFO:Total runtime is 31.951778495311736 minutes
2024-05-12 15:52:09,586:INFO:SubProcess create_model() called ==================================
2024-05-12 15:52:09,586:INFO:Initializing create_model()
2024-05-12 15:52:09,586:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002879ABB57D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 15:52:09,587:INFO:Checking exceptions
2024-05-12 15:52:09,587:INFO:Importing libraries
2024-05-12 15:52:09,587:INFO:Copying training dataset
2024-05-12 15:52:11,134:INFO:Defining folds
2024-05-12 15:52:11,134:INFO:Declaring metric variables
2024-05-12 15:52:11,142:INFO:Importing untrained model
2024-05-12 15:52:11,142:INFO:Random Forest Classifier Imported successfully
2024-05-12 15:52:11,148:INFO:Starting cross validation
2024-05-12 15:52:11,166:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 16:05:48,218:INFO:Calculating mean and std
2024-05-12 16:05:48,218:INFO:Creating metrics dataframe
2024-05-12 16:05:48,230:INFO:Uploading results into container
2024-05-12 16:05:48,230:INFO:Uploading model into container now
2024-05-12 16:05:48,230:INFO:_master_model_container: 7
2024-05-12 16:05:48,231:INFO:_display_container: 2
2024-05-12 16:05:48,232:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=2,
                       oob_score=False, random_state=3090, verbose=0,
                       warm_start=False)
2024-05-12 16:05:48,232:INFO:create_model() successfully completed......................................
2024-05-12 16:05:48,399:INFO:SubProcess create_model() end ==================================
2024-05-12 16:05:48,399:INFO:Creating metrics dataframe
2024-05-12 16:05:48,415:INFO:Initializing Quadratic Discriminant Analysis
2024-05-12 16:05:48,415:INFO:Total runtime is 45.599067684014635 minutes
2024-05-12 16:05:48,419:INFO:SubProcess create_model() called ==================================
2024-05-12 16:05:48,419:INFO:Initializing create_model()
2024-05-12 16:05:48,419:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002879ABB57D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 16:05:48,419:INFO:Checking exceptions
2024-05-12 16:05:48,419:INFO:Importing libraries
2024-05-12 16:05:48,419:INFO:Copying training dataset
2024-05-12 16:05:49,757:INFO:Defining folds
2024-05-12 16:05:49,757:INFO:Declaring metric variables
2024-05-12 16:05:49,762:INFO:Importing untrained model
2024-05-12 16:05:49,764:INFO:Quadratic Discriminant Analysis Imported successfully
2024-05-12 16:05:49,773:INFO:Starting cross validation
2024-05-12 16:05:49,788:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 16:06:05,926:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 16:06:06,538:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 16:06:11,238:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:06:12,093:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:06:26,953:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 16:06:27,376:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 16:06:32,657:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:06:33,342:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:06:48,515:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 16:06:48,925:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 16:06:54,099:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:06:54,960:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:07:10,244:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 16:07:10,917:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 16:07:15,838:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:07:16,535:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:07:31,107:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 16:07:31,716:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 16:07:36,854:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:07:37,389:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:07:37,409:INFO:Calculating mean and std
2024-05-12 16:07:37,411:INFO:Creating metrics dataframe
2024-05-12 16:07:37,416:INFO:Uploading results into container
2024-05-12 16:07:37,417:INFO:Uploading model into container now
2024-05-12 16:07:37,417:INFO:_master_model_container: 8
2024-05-12 16:07:37,417:INFO:_display_container: 2
2024-05-12 16:07:37,417:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-05-12 16:07:37,417:INFO:create_model() successfully completed......................................
2024-05-12 16:07:37,617:INFO:SubProcess create_model() end ==================================
2024-05-12 16:07:37,618:INFO:Creating metrics dataframe
2024-05-12 16:07:37,630:INFO:Initializing Ada Boost Classifier
2024-05-12 16:07:37,633:INFO:Total runtime is 47.419355972607924 minutes
2024-05-12 16:07:37,636:INFO:SubProcess create_model() called ==================================
2024-05-12 16:07:37,636:INFO:Initializing create_model()
2024-05-12 16:07:37,636:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002879ABB57D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 16:07:37,636:INFO:Checking exceptions
2024-05-12 16:07:37,636:INFO:Importing libraries
2024-05-12 16:07:37,636:INFO:Copying training dataset
2024-05-12 16:07:38,917:INFO:Defining folds
2024-05-12 16:07:38,917:INFO:Declaring metric variables
2024-05-12 16:07:38,917:INFO:Importing untrained model
2024-05-12 16:07:38,926:INFO:Ada Boost Classifier Imported successfully
2024-05-12 16:07:38,931:INFO:Starting cross validation
2024-05-12 16:07:38,944:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 16:07:53,190:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-12 16:07:53,361:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-12 16:14:38,254:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:14:47,817:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-12 16:15:17,197:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:15:26,927:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-12 16:21:28,500:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:21:38,160:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-12 16:22:08,174:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:22:17,384:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-12 16:28:20,574:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:28:30,188:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-12 16:29:03,893:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:29:13,101:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-12 16:35:14,044:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:35:24,077:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-12 16:36:00,050:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:36:09,250:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-05-12 16:42:11,880:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:42:50,449:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 16:42:50,469:INFO:Calculating mean and std
2024-05-12 16:42:50,471:INFO:Creating metrics dataframe
2024-05-12 16:42:50,476:INFO:Uploading results into container
2024-05-12 16:42:50,476:INFO:Uploading model into container now
2024-05-12 16:42:50,476:INFO:_master_model_container: 9
2024-05-12 16:42:50,476:INFO:_display_container: 2
2024-05-12 16:42:50,476:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=3090)
2024-05-12 16:42:50,476:INFO:create_model() successfully completed......................................
2024-05-12 16:42:50,687:INFO:SubProcess create_model() end ==================================
2024-05-12 16:42:50,687:INFO:Creating metrics dataframe
2024-05-12 16:42:50,698:INFO:Initializing Gradient Boosting Classifier
2024-05-12 16:42:50,698:INFO:Total runtime is 82.6371210416158 minutes
2024-05-12 16:42:50,698:INFO:SubProcess create_model() called ==================================
2024-05-12 16:42:50,698:INFO:Initializing create_model()
2024-05-12 16:42:50,698:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002879ABB57D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 16:42:50,698:INFO:Checking exceptions
2024-05-12 16:42:50,698:INFO:Importing libraries
2024-05-12 16:42:50,698:INFO:Copying training dataset
2024-05-12 16:42:52,070:INFO:Defining folds
2024-05-12 16:42:52,070:INFO:Declaring metric variables
2024-05-12 16:42:52,075:INFO:Importing untrained model
2024-05-12 16:42:52,078:INFO:Gradient Boosting Classifier Imported successfully
2024-05-12 16:42:52,086:INFO:Starting cross validation
2024-05-12 16:42:52,101:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 18:57:46,549:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 18:57:47,966:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 20:52:43,076:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 20:53:23,247:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 20:58:05,242:INFO:Initializing create_model()
2024-05-12 20:58:05,243:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 20:58:05,243:INFO:Checking exceptions
2024-05-12 20:58:05,267:INFO:Importing libraries
2024-05-12 20:58:05,267:INFO:Copying training dataset
2024-05-12 20:58:06,838:INFO:Defining folds
2024-05-12 20:58:06,838:INFO:Declaring metric variables
2024-05-12 20:58:06,838:INFO:Importing untrained model
2024-05-12 20:58:06,844:INFO:Random Forest Classifier Imported successfully
2024-05-12 20:58:06,848:INFO:Starting cross validation
2024-05-12 20:58:06,936:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 21:08:36,414:INFO:Calculating mean and std
2024-05-12 21:08:36,420:INFO:Creating metrics dataframe
2024-05-12 21:08:36,435:INFO:Finalizing model
2024-05-12 21:10:48,026:INFO:Uploading results into container
2024-05-12 21:10:48,027:INFO:Uploading model into container now
2024-05-12 21:10:48,051:INFO:_master_model_container: 10
2024-05-12 21:10:48,051:INFO:_display_container: 2
2024-05-12 21:10:48,053:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=2,
                       oob_score=False, random_state=3090, verbose=0,
                       warm_start=False)
2024-05-12 21:10:48,053:INFO:create_model() successfully completed......................................
2024-05-12 21:13:43,484:INFO:Initializing plot_model()
2024-05-12 21:13:43,484:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=2,
                       oob_score=False, random_state=3090, verbose=0,
                       warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-05-12 21:13:43,484:INFO:Checking exceptions
2024-05-12 21:13:44,010:INFO:Preloading libraries
2024-05-12 21:13:44,061:INFO:Copying training dataset
2024-05-12 21:13:44,061:INFO:Plot type: confusion_matrix
2024-05-12 21:13:47,742:INFO:Fitting Model
2024-05-12 21:13:47,743:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names
  warnings.warn(

2024-05-12 21:13:47,745:INFO:Scoring test/hold-out set
2024-05-12 21:13:48,764:INFO:Visual Rendered Successfully
2024-05-12 21:13:48,906:INFO:plot_model() successfully completed......................................
2024-05-12 21:14:05,752:INFO:Initializing plot_model()
2024-05-12 21:14:05,752:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=2,
                       oob_score=False, random_state=3090, verbose=0,
                       warm_start=False), plot=boundary, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-05-12 21:14:05,752:INFO:Checking exceptions
2024-05-12 21:14:06,145:INFO:Preloading libraries
2024-05-12 21:14:06,192:INFO:Copying training dataset
2024-05-12 21:14:06,192:INFO:Plot type: boundary
2024-05-12 21:14:08,131:INFO:Fitting StandardScaler()
2024-05-12 21:14:09,132:INFO:Fitting PCA()
2024-05-12 21:14:13,192:INFO:Fitting Model
2024-05-12 21:14:22,371:INFO:Visual Rendered Successfully
2024-05-12 21:14:22,565:INFO:plot_model() successfully completed......................................
2024-05-12 21:14:28,385:INFO:Initializing plot_model()
2024-05-12 21:14:28,385:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=2,
                       oob_score=False, random_state=3090, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-05-12 21:14:28,385:INFO:Checking exceptions
2024-05-12 21:14:28,836:INFO:Preloading libraries
2024-05-12 21:14:28,893:INFO:Copying training dataset
2024-05-12 21:14:28,893:INFO:Plot type: feature
2024-05-12 21:14:28,894:WARNING:No coef_ found. Trying feature_importances_
2024-05-12 21:14:30,107:INFO:Visual Rendered Successfully
2024-05-12 21:14:30,252:INFO:plot_model() successfully completed......................................
2024-05-12 21:15:48,001:INFO:Initializing save_model()
2024-05-12 21:15:48,001:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=2,
                       oob_score=False, random_state=3090, verbose=0,
                       warm_start=False), model_name=best-model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\MANVEN~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-05-12 21:15:48,001:INFO:Adding model into prep_pipe
2024-05-12 21:16:00,176:INFO:Initializing save_model()
2024-05-12 21:16:00,176:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=2,
                       oob_score=False, random_state=3090, verbose=0,
                       warm_start=False), model_name=F:\RESTCN_CODE\best-model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\MANVEN~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-05-12 21:16:00,176:INFO:Adding model into prep_pipe
2024-05-12 21:16:00,688:INFO:F:\RESTCN_CODE\best-model.pkl saved in current working directory
2024-05-12 21:16:00,698:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=2, oob_score=False,
                                        random_state=3090, verbose=0,
                                        warm_start=False))],
         verbose=False)
2024-05-12 21:16:00,698:INFO:save_model() successfully completed......................................
2024-05-12 21:21:40,548:INFO:Initializing predict_model()
2024-05-12 21:21:40,548:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=2,
                       oob_score=False, random_state=3090, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002879B475EE0>)
2024-05-12 21:21:40,548:INFO:Checking exceptions
2024-05-12 21:21:40,548:INFO:Preloading libraries
2024-05-12 21:21:40,550:INFO:Set up data.
2024-05-12 21:21:40,731:INFO:Set up index.
2024-05-12 21:21:42,451:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-12 21:22:59,060:INFO:Initializing create_model()
2024-05-12 21:22:59,060:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=qda, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 21:22:59,060:INFO:Checking exceptions
2024-05-12 21:22:59,069:INFO:Importing libraries
2024-05-12 21:22:59,069:INFO:Copying training dataset
2024-05-12 21:23:00,324:INFO:Defining folds
2024-05-12 21:23:00,324:INFO:Declaring metric variables
2024-05-12 21:23:00,324:INFO:Importing untrained model
2024-05-12 21:23:00,331:INFO:Quadratic Discriminant Analysis Imported successfully
2024-05-12 21:23:00,338:INFO:Starting cross validation
2024-05-12 21:23:00,353:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 21:23:20,233:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 21:23:20,246:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 21:23:25,033:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 21:23:25,161:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 21:23:38,961:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 21:23:39,600:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 21:23:43,578:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 21:23:44,108:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 21:23:57,635:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 21:23:58,192:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 21:24:02,255:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 21:24:02,780:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 21:24:16,045:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 21:24:16,663:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 21:24:20,599:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 21:24:21,141:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 21:24:34,244:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 21:24:34,886:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 21:24:38,855:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 21:24:39,307:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 21:24:39,321:INFO:Calculating mean and std
2024-05-12 21:24:39,321:INFO:Creating metrics dataframe
2024-05-12 21:24:39,331:INFO:Finalizing model
2024-05-12 21:24:48,576:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-05-12 21:24:51,578:INFO:Uploading results into container
2024-05-12 21:24:51,579:INFO:Uploading model into container now
2024-05-12 21:24:51,586:INFO:_master_model_container: 11
2024-05-12 21:24:51,591:INFO:_display_container: 4
2024-05-12 21:24:51,591:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-05-12 21:24:51,591:INFO:create_model() successfully completed......................................
2024-05-12 21:24:51,832:INFO:Initializing predict_model()
2024-05-12 21:24:51,832:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000287EE4A9D00>)
2024-05-12 21:24:51,832:INFO:Checking exceptions
2024-05-12 21:24:51,832:INFO:Preloading libraries
2024-05-12 21:24:51,832:INFO:Set up data.
2024-05-12 21:24:52,036:INFO:Set up index.
2024-05-12 21:24:53,084:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.AxisError: axis 1 is out of bounds for array of dimension 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.AxisError: axis 1 is out of bounds for array of dimension 1

  warnings.warn(traceback.format_exc())

2024-05-12 21:24:53,098:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-05-12 21:25:28,271:INFO:Initializing create_model()
2024-05-12 21:25:28,271:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=knn, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 21:25:28,314:INFO:Checking exceptions
2024-05-12 21:25:28,327:INFO:Importing libraries
2024-05-12 21:25:28,327:INFO:Copying training dataset
2024-05-12 21:25:29,528:INFO:Defining folds
2024-05-12 21:25:29,528:INFO:Declaring metric variables
2024-05-12 21:25:29,538:INFO:Importing untrained model
2024-05-12 21:25:29,540:INFO:K Neighbors Classifier Imported successfully
2024-05-12 21:25:29,543:INFO:Starting cross validation
2024-05-12 21:25:29,556:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 21:28:16,796:INFO:Calculating mean and std
2024-05-12 21:28:16,798:INFO:Creating metrics dataframe
2024-05-12 21:28:16,805:INFO:Finalizing model
2024-05-12 21:28:25,581:INFO:Uploading results into container
2024-05-12 21:28:25,581:INFO:Uploading model into container now
2024-05-12 21:28:25,587:INFO:_master_model_container: 12
2024-05-12 21:28:25,587:INFO:_display_container: 6
2024-05-12 21:28:25,587:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=2, n_neighbors=5, p=2,
                     weights='uniform')
2024-05-12 21:28:25,587:INFO:create_model() successfully completed......................................
2024-05-12 21:28:25,797:INFO:Initializing predict_model()
2024-05-12 21:28:25,797:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=2, n_neighbors=5, p=2,
                     weights='uniform'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002879B474220>)
2024-05-12 21:28:25,797:INFO:Checking exceptions
2024-05-12 21:28:25,797:INFO:Preloading libraries
2024-05-12 21:28:25,797:INFO:Set up data.
2024-05-12 21:28:25,982:INFO:Set up index.
2024-05-12 22:59:46,013:INFO:Initializing create_model()
2024-05-12 22:59:46,013:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=ridge, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-05-12 22:59:46,013:INFO:Checking exceptions
2024-05-12 22:59:46,035:INFO:Importing libraries
2024-05-12 22:59:46,035:INFO:Copying training dataset
2024-05-12 22:59:47,696:INFO:Defining folds
2024-05-12 22:59:47,710:INFO:Declaring metric variables
2024-05-12 22:59:47,712:INFO:Importing untrained model
2024-05-12 22:59:47,718:INFO:Ridge Classifier Imported successfully
2024-05-12 22:59:47,718:INFO:Starting cross validation
2024-05-12 22:59:47,762:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=2
2024-05-12 23:00:06,218:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.38224e-10): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-05-12 23:00:06,308:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.96614e-10): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-05-12 23:00:06,431:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 23:00:06,509:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 23:00:20,734:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=1.02021e-10): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-05-12 23:00:21,026:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 23:00:22,606:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 23:00:37,337:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 23:00:38,693:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 23:00:54,702:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 23:00:55,707:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 23:01:10,453:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 23:01:11,112:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-05-12 23:01:11,127:INFO:Calculating mean and std
2024-05-12 23:01:11,143:INFO:Creating metrics dataframe
2024-05-12 23:01:11,159:INFO:Finalizing model
2024-05-12 23:01:22,102:INFO:Uploading results into container
2024-05-12 23:01:22,102:INFO:Uploading model into container now
2024-05-12 23:01:22,131:INFO:_master_model_container: 13
2024-05-12 23:01:22,131:INFO:_display_container: 8
2024-05-12 23:01:22,131:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=3090, solver='auto',
                tol=0.0001)
2024-05-12 23:01:22,131:INFO:create_model() successfully completed......................................
2024-05-12 23:01:22,792:INFO:Initializing predict_model()
2024-05-12 23:01:22,792:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000287FBF30D50>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=3090, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002879B4772E0>)
2024-05-12 23:01:22,792:INFO:Checking exceptions
2024-05-12 23:01:22,792:INFO:Preloading libraries
2024-05-12 23:01:22,792:INFO:Set up data.
2024-05-12 23:01:23,213:INFO:Set up index.
2024-05-12 23:01:23,557:WARNING:c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.AxisError: axis 1 is out of bounds for array of dimension 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Manvendra Nema\anaconda3\envs\vercil\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.AxisError: axis 1 is out of bounds for array of dimension 1

  warnings.warn(traceback.format_exc())

2024-05-13 00:40:21,122:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-13 00:40:21,167:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-13 00:40:21,167:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-13 00:40:21,167:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-05-13 00:40:25,877:INFO:Initializing load_model()
2024-05-13 00:40:25,878:INFO:load_model(model_name=F:\RESTCN_CODEest-model.pkl, platform=None, authentication=None, verbose=True)
2024-05-13 00:40:33,109:INFO:Initializing load_model()
2024-05-13 00:40:33,109:INFO:load_model(model_name=F:\RESTCN_CODE\best-model.pkl, platform=None, authentication=None, verbose=True)
2024-05-13 00:40:40,073:INFO:Initializing load_model()
2024-05-13 00:40:40,073:INFO:load_model(model_name=F:\RESTCN_CODE\best-model, platform=None, authentication=None, verbose=True)
2024-05-13 00:47:10,824:INFO:Initializing load_model()
2024-05-13 00:47:10,824:INFO:load_model(model_name=F:\RESTCN_CODE\best-model, platform=None, authentication=None, verbose=True)
2024-05-13 00:47:11,000:INFO:Initializing predict_model()
2024-05-13 00:47:11,001:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023FF924B9D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('balance',
                 TransformerWrapper(transformer=FixImbalancer(estimator=SMOTE()))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=2, random_state=3090))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000023FE4619B20>)
2024-05-13 00:47:11,001:INFO:Checking exceptions
2024-05-13 00:47:11,001:INFO:Preloading libraries
2024-05-13 00:47:11,002:INFO:Set up data.
2024-05-13 00:47:11,262:INFO:Set up index.
