{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, addr, img_addr, transform=None):\n",
    "        self.file_name = sorted(os.listdir(addr))\n",
    "        self.img_addr = img_addr\n",
    "        self.addr = addr\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = np.array(pd.read_csv(os.path.join(self.addr, self.file_name[idx], self.file_name[idx] + '.csv')).iloc[:, 5:])\n",
    "        img_path = os.path.join(self.img_addr, self.file_name[idx] + '.jpg')\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        targ = int(self.file_name[idx].split('_')[-1])  # Convert target to integer\n",
    "\n",
    "        return torch.tensor(features).squeeze(0).float(), img, targ\n",
    "\n",
    "# Transformation for images to resize and normalize for ResNet18\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize image to fit ResNet18 input size\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for ResNet18\n",
    "])\n",
    "\n",
    "# Example usage:\n",
    "# dataset = SimpleDataset(addr='path_to_csv_files', img_addr='path_to_images', transform=image_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr =    r\"F:\\Frames\\extracted\\Train\"\n",
    "img_addr = r\"F:\\Frames\\new_dataset\\Train_Frames\\Train_Frames\"\n",
    "train = SimpleDataset(addr,img_addr, transform=image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr =    r\"F:\\Frames\\extracted\\Test\"\n",
    "img_addr = r\"F:\\Frames\\new_dataset\\Test_Frames\\Test_Frames\"\n",
    "test = SimpleDataset(addr,img_addr,transform=image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "addr =    r\"F:\\Frames\\extracted\\Val\"\n",
    "img_addr = r\"F:\\Frames\\new_dataset\\Validation_Frames\\Validation_Frames\"\n",
    "val = SimpleDataset(addr,img_addr, transform=image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64  # You can adjust this according to your system's memory capacity\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Load pre-trained ResNet18 model\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        # Change the output layer to match the number of classes in your dataset\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Define additional fully connected layers\n",
    "        self.fc1 = nn.Linear(num_features + 713, 32)  # Adjusted input size\n",
    "        self.bn1 = nn.BatchNorm1d(32)  # Batch normalization\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Dropout regularization\n",
    "\n",
    "        self.fc2 = nn.Linear(32, 8)\n",
    "        self.bn2 = nn.BatchNorm1d(8)  # Batch normalization\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Dropout regularization\n",
    "\n",
    "        self.fc3 = nn.Linear(8, num_classes)\n",
    "\n",
    "    def forward(self, features, images):\n",
    "        # Extract features from images using ResNet18\n",
    "        img_features = self.resnet(images)\n",
    "\n",
    "        # Concatenate image features with additional features\n",
    "        combined_features = torch.cat((img_features, features), dim=1)\n",
    "\n",
    "        # Pass through additional fully connected layers with batch normalization and dropout\n",
    "        x = F.relu(self.fc1(combined_features))\n",
    "        x = self.bn1(x)  # Batch normalization\n",
    "        x = self.dropout1(x)  # Dropout regularization\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)  # Batch normalization\n",
    "        x = self.dropout2(x)  # Dropout regularization\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "# Define the number of classes in your dataset\n",
    "num_classes = 4  # Example value, replace with the actual number of classes\n",
    "model = MyModel(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55485053 0.22259234 0.55555556 0.11261771]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix = np.array([[0, 0, 40, 0],\n",
    "                              [0, 0, 840, 0],\n",
    "                              [0, 0, 8820, 0],\n",
    "                              [0, 0, 8140, 0]])\n",
    "\n",
    "# Calculate class frequencies\n",
    "class_frequencies = np.sum(confusion_matrix, axis=1) / np.sum(confusion_matrix)\n",
    "\n",
    "# Compute weights based on class frequencies\n",
    "weights = 1 / class_frequencies\n",
    "\n",
    "# Normalize weights to have a maximum value of 1\n",
    "max_weight = np.max(weights)\n",
    "weights = weights / max_weight\n",
    "\n",
    "# Apply smoothing\n",
    "smoothing_factor = 0.3\n",
    "weights = np.power(weights, smoothing_factor)\n",
    "\n",
    "# Normalize weights again to ensure they sum to 1\n",
    "weights = weights / np.sum(weights)\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Train: 100%|██████████| 1236/1236 [29:20<00:00,  1.42s/it, loss=1.25]   \n",
      "Epoch 1/6, Validation: 100%|██████████| 269/269 [05:42<00:00,  1.27s/it, loss=1.31]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Train Loss: 2.9666, Train Accuracy: 49.57%, Validation Loss: 323.9690, Validation Accuracy: 52.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6, Train: 100%|██████████| 1236/1236 [28:49<00:00,  1.40s/it, loss=0.953]\n",
      "Epoch 2/6, Validation: 100%|██████████| 269/269 [06:11<00:00,  1.38s/it, loss=1.39]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/6, Train Loss: 1.0689, Train Accuracy: 50.52%, Validation Loss: 334.2277, Validation Accuracy: 52.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6, Train: 100%|██████████| 1236/1236 [28:18<00:00,  1.37s/it, loss=1.19] \n",
      "Epoch 3/6, Validation: 100%|██████████| 269/269 [05:42<00:00,  1.28s/it, loss=1.4]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/6, Train Loss: 1.0003, Train Accuracy: 50.52%, Validation Loss: 322.0990, Validation Accuracy: 52.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6, Train:  12%|█▏        | 145/1236 [03:15<24:27,  1.35s/it, loss=1.24] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Wrap train_loader with tqdm\u001b[39;00m\n\u001b[0;32m     30\u001b[0m train_loader_iter \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader_iter:\n\u001b[0;32m     33\u001b[0m     features, images, labels \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mto(device), images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move to GPU\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m, in \u001b[0;36mSimpleDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 24\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m     26\u001b[0m targ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_name[idx]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Convert target to integer\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(features)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), img, targ\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresize(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torchvision\\transforms\\functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mresize(\u001b[38;5;28mtuple\u001b[39m(size[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), interpolation)\n",
      "File \u001b[1;32mc:\\Users\\Manvendra Nema\\anaconda3\\envs\\vercil\\Lib\\site-packages\\PIL\\Image.py:2200\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2192\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2193\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2194\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2195\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2196\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2197\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2198\u001b[0m         )\n\u001b[1;32m-> 2200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mresize(size, resample, box))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define your model, optimizer, loss function, and other parameters\n",
    "num_classes = 4  # Example value, replace with the actual number of classes\n",
    "\n",
    "model = model.to(device)\n",
    "criterion =  torch.nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float32,device='cuda'))\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 6\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Wrap train_loader with tqdm\n",
    "    train_loader_iter = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}, Train')\n",
    "    \n",
    "    for features, images, labels in train_loader_iter:\n",
    "        features, images, labels = features.to(device), images.to(device), labels.to(device)  # Move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(features, images)\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * features.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Update tqdm description\n",
    "        train_loader_iter.set_postfix(loss=loss.item())\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Wrap val_loader with tqdm\n",
    "    val_loader_iter = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs}, Validation')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, images, labels in val_loader_iter:\n",
    "            features, images, labels = features.to(device), images.to(device), labels.to(device)  # Move to GPU\n",
    "            # Forward pass\n",
    "            outputs = model(features, images)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item() * features.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update tqdm description\n",
    "            val_loader_iter.set_postfix(loss=loss.item())\n",
    "    \n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path where the model is saved\n",
    "model_path = r\"F:\\RESTCN_CODE\\frame-feature.pth\"\n",
    "\n",
    "\n",
    "# # Save the model\n",
    "torch.save(model.state_dict(), model_path)\n",
    "# Define your model architecture\n",
    "# Example:\n",
    "# model = YourModelClass()\n",
    "\n",
    "# Load the saved model parameters\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# It's important to call model.eval() after loading the model\n",
    "# if you're going to use it for inference\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 279/279 [06:06<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.49\n",
      "Test F1 Score: 0.17\n",
      "Confusion Matrix:\n",
      "[[   0    0   40    0]\n",
      " [   0    0  826   14]\n",
      " [   0    0 8819    1]\n",
      " [   0    0 8138    2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "predictions = [] \n",
    "true_labels = []\n",
    "\n",
    "# Wrap test_loader with tqdm\n",
    "test_loader_iter = tqdm(test_loader, desc=f'Testing')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, images, labels in test_loader_iter:\n",
    "        features, images, labels = features.to(device), images.to(device), labels.to(device)  # Move to GPU\n",
    "        # Forward pass\n",
    "        outputs = model(features, images)\n",
    "        # Get predicted labels\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # Collect predictions and true labels\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(true_labels, predictions)\n",
    "# Calculate F1 score\n",
    "test_f1_score = f1_score(true_labels, predictions, average='macro')\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "print(f'Test F1 Score: {test_f1_score:.2f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vercil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
